{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup\n",
        "\n",
        "Ensure all dependencies are installed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "scrolled": true,
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch/, https://download.pytorch.org/whl/torchvision/\n",
            "Collecting pointnet (from -r requirements.txt (line 20))\n",
            "  Cloning https://github.com/fxia22/pointnet.pytorch.git (to revision master) to /tmp/pip-install-2sa27mga/pointnet_232b62d7fb664c20b8ea6911589107f5\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/fxia22/pointnet.pytorch.git /tmp/pip-install-2sa27mga/pointnet_232b62d7fb664c20b8ea6911589107f5\n",
            "  Resolved https://github.com/fxia22/pointnet.pytorch.git to commit bafbf401e0af19be3262e448c59313fd2be0e421\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: torch==2.0.1+cu117 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (2.0.1+cu117)\n",
            "Requirement already satisfied: torchvision==0.15.2+cu117 in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.15.2+cu117)\n",
            "Requirement already satisfied: tensorboard in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (2.18.0)\n",
            "Collecting torch-tb-profiler (from -r requirements.txt (line 7))\n",
            "  Downloading torch_tb_profiler-0.4.3-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: tqdm in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (4.67.1)\n",
            "Requirement already satisfied: numpy in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (1.26.4)\n",
            "Collecting numpy (from -r requirements.txt (line 10))\n",
            "  Using cached numpy-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Requirement already satisfied: pandas in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (2.2.3)\n",
            "Requirement already satisfied: plyfile in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (1.1)\n",
            "Requirement already satisfied: requests in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (2.32.3)\n",
            "Requirement already satisfied: open3d in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (0.19.0)\n",
            "Requirement already satisfied: matplotlib in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 15)) (3.10.0)\n",
            "Requirement already satisfied: segments-ai in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 16)) (1.11.11)\n",
            "Requirement already satisfied: python-dotenv in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 17)) (1.0.1)\n",
            "Requirement already satisfied: lightning in ./.conda/lib/python3.10/site-packages (from -r requirements.txt (line 18)) (2.3.3)\n",
            "Collecting lightning (from -r requirements.txt (line 18))\n",
            "  Using cached lightning-2.5.0.post0-py3-none-any.whl.metadata (40 kB)\n",
            "Requirement already satisfied: filelock in ./.conda/lib/python3.10/site-packages (from torch==2.0.1+cu117->-r requirements.txt (line 3)) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in ./.conda/lib/python3.10/site-packages (from torch==2.0.1+cu117->-r requirements.txt (line 3)) (4.12.2)\n",
            "Requirement already satisfied: sympy in ./.conda/lib/python3.10/site-packages (from torch==2.0.1+cu117->-r requirements.txt (line 3)) (1.13.3)\n",
            "Requirement already satisfied: networkx in ./.conda/lib/python3.10/site-packages (from torch==2.0.1+cu117->-r requirements.txt (line 3)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in ./.conda/lib/python3.10/site-packages (from torch==2.0.1+cu117->-r requirements.txt (line 3)) (3.1.5)\n",
            "Requirement already satisfied: triton==2.0.0 in ./.conda/lib/python3.10/site-packages (from torch==2.0.1+cu117->-r requirements.txt (line 3)) (2.0.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.conda/lib/python3.10/site-packages (from torchvision==0.15.2+cu117->-r requirements.txt (line 5)) (11.0.0)\n",
            "Requirement already satisfied: cmake in ./.conda/lib/python3.10/site-packages (from triton==2.0.0->torch==2.0.1+cu117->-r requirements.txt (line 3)) (3.31.2)\n",
            "Requirement already satisfied: lit in ./.conda/lib/python3.10/site-packages (from triton==2.0.0->torch==2.0.1+cu117->-r requirements.txt (line 3)) (18.1.8)\n",
            "Requirement already satisfied: absl-py>=0.4 in ./.conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 6)) (2.1.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in ./.conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 6)) (1.68.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in ./.conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 6)) (3.7)\n",
            "Requirement already satisfied: packaging in ./.conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 6)) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in ./.conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 6)) (5.29.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in ./.conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 6)) (75.6.0)\n",
            "Requirement already satisfied: six>1.9 in ./.conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 6)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./.conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 6)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in ./.conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 6)) (3.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./.conda/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 11)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./.conda/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 11)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in ./.conda/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 11)) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.10/site-packages (from requests->-r requirements.txt (line 13)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.10/site-packages (from requests->-r requirements.txt (line 13)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.10/site-packages (from requests->-r requirements.txt (line 13)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.10/site-packages (from requests->-r requirements.txt (line 13)) (2024.12.14)\n",
            "Requirement already satisfied: dash>=2.6.0 in ./.conda/lib/python3.10/site-packages (from open3d->-r requirements.txt (line 14)) (2.18.2)\n",
            "Requirement already satisfied: flask>=3.0.0 in ./.conda/lib/python3.10/site-packages (from open3d->-r requirements.txt (line 14)) (3.0.3)\n",
            "Requirement already satisfied: nbformat>=5.7.0 in ./.conda/lib/python3.10/site-packages (from open3d->-r requirements.txt (line 14)) (5.10.4)\n",
            "Requirement already satisfied: configargparse in ./.conda/lib/python3.10/site-packages (from open3d->-r requirements.txt (line 14)) (1.7)\n",
            "Requirement already satisfied: ipywidgets>=8.0.4 in ./.conda/lib/python3.10/site-packages (from open3d->-r requirements.txt (line 14)) (8.1.5)\n",
            "Requirement already satisfied: addict in ./.conda/lib/python3.10/site-packages (from open3d->-r requirements.txt (line 14)) (2.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.4.1 in ./.conda/lib/python3.10/site-packages (from open3d->-r requirements.txt (line 14)) (6.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21 in ./.conda/lib/python3.10/site-packages (from open3d->-r requirements.txt (line 14)) (1.6.0)\n",
            "Requirement already satisfied: pyquaternion in ./.conda/lib/python3.10/site-packages (from open3d->-r requirements.txt (line 14)) (0.9.9)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in ./.conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 15)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in ./.conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 15)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in ./.conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 15)) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in ./.conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 15)) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in ./.conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 15)) (3.2.0)\n",
            "Requirement already satisfied: pydantic!=2.4.0,==2.* in ./.conda/lib/python3.10/site-packages (from segments-ai->-r requirements.txt (line 16)) (2.10.4)\n",
            "Requirement already satisfied: types-Pillow>=9.0 in ./.conda/lib/python3.10/site-packages (from segments-ai->-r requirements.txt (line 16)) (10.2.0.20240822)\n",
            "Requirement already satisfied: types-requests==2.* in ./.conda/lib/python3.10/site-packages (from segments-ai->-r requirements.txt (line 16)) (2.32.0.20241016)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in ./.conda/lib/python3.10/site-packages (from pydantic!=2.4.0,==2.*->segments-ai->-r requirements.txt (line 16)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in ./.conda/lib/python3.10/site-packages (from pydantic!=2.4.0,==2.*->segments-ai->-r requirements.txt (line 16)) (2.27.2)\n",
            "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in ./.conda/lib/python3.10/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning->-r requirements.txt (line 18)) (2024.12.0)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in ./.conda/lib/python3.10/site-packages (from lightning->-r requirements.txt (line 18)) (0.11.9)\n",
            "INFO: pip is looking at multiple versions of lightning to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached lightning-2.5.0-py3-none-any.whl.metadata (40 kB)\n",
            "  Using cached lightning-2.4.0-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in ./.conda/lib/python3.10/site-packages (from lightning->-r requirements.txt (line 18)) (1.6.1)\n",
            "Requirement already satisfied: pytorch-lightning in ./.conda/lib/python3.10/site-packages (from lightning->-r requirements.txt (line 18)) (2.3.3)\n",
            "Requirement already satisfied: plotly>=5.0.0 in ./.conda/lib/python3.10/site-packages (from dash>=2.6.0->open3d->-r requirements.txt (line 14)) (5.24.1)\n",
            "Requirement already satisfied: dash-html-components==2.0.0 in ./.conda/lib/python3.10/site-packages (from dash>=2.6.0->open3d->-r requirements.txt (line 14)) (2.0.0)\n",
            "Requirement already satisfied: dash-core-components==2.0.0 in ./.conda/lib/python3.10/site-packages (from dash>=2.6.0->open3d->-r requirements.txt (line 14)) (2.0.0)\n",
            "Requirement already satisfied: dash-table==5.0.0 in ./.conda/lib/python3.10/site-packages (from dash>=2.6.0->open3d->-r requirements.txt (line 14)) (5.0.0)\n",
            "Requirement already satisfied: importlib-metadata in ./.conda/lib/python3.10/site-packages (from dash>=2.6.0->open3d->-r requirements.txt (line 14)) (8.5.0)\n",
            "Requirement already satisfied: retrying in ./.conda/lib/python3.10/site-packages (from dash>=2.6.0->open3d->-r requirements.txt (line 14)) (1.3.4)\n",
            "Requirement already satisfied: nest-asyncio in ./.conda/lib/python3.10/site-packages (from dash>=2.6.0->open3d->-r requirements.txt (line 14)) (1.6.0)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in ./.conda/lib/python3.10/site-packages (from flask>=3.0.0->open3d->-r requirements.txt (line 14)) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in ./.conda/lib/python3.10/site-packages (from flask>=3.0.0->open3d->-r requirements.txt (line 14)) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.6.2 in ./.conda/lib/python3.10/site-packages (from flask>=3.0.0->open3d->-r requirements.txt (line 14)) (1.9.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.conda/lib/python3.10/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning->-r requirements.txt (line 18)) (3.11.11)\n",
            "Requirement already satisfied: comm>=0.1.3 in ./.conda/lib/python3.10/site-packages (from ipywidgets>=8.0.4->open3d->-r requirements.txt (line 14)) (0.2.2)\n",
            "Requirement already satisfied: ipython>=6.1.0 in ./.conda/lib/python3.10/site-packages (from ipywidgets>=8.0.4->open3d->-r requirements.txt (line 14)) (8.31.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in ./.conda/lib/python3.10/site-packages (from ipywidgets>=8.0.4->open3d->-r requirements.txt (line 14)) (5.14.3)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.12 in ./.conda/lib/python3.10/site-packages (from ipywidgets>=8.0.4->open3d->-r requirements.txt (line 14)) (4.0.13)\n",
            "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in ./.conda/lib/python3.10/site-packages (from ipywidgets>=8.0.4->open3d->-r requirements.txt (line 14)) (3.0.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.conda/lib/python3.10/site-packages (from jinja2->torch==2.0.1+cu117->-r requirements.txt (line 3)) (3.0.2)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in ./.conda/lib/python3.10/site-packages (from nbformat>=5.7.0->open3d->-r requirements.txt (line 14)) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in ./.conda/lib/python3.10/site-packages (from nbformat>=5.7.0->open3d->-r requirements.txt (line 14)) (4.23.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in ./.conda/lib/python3.10/site-packages (from nbformat>=5.7.0->open3d->-r requirements.txt (line 14)) (5.7.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in ./.conda/lib/python3.10/site-packages (from scikit-learn>=0.21->open3d->-r requirements.txt (line 14)) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in ./.conda/lib/python3.10/site-packages (from scikit-learn>=0.21->open3d->-r requirements.txt (line 14)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.conda/lib/python3.10/site-packages (from scikit-learn>=0.21->open3d->-r requirements.txt (line 14)) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.conda/lib/python3.10/site-packages (from sympy->torch==2.0.1+cu117->-r requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning->-r requirements.txt (line 18)) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in ./.conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning->-r requirements.txt (line 18)) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning->-r requirements.txt (line 18)) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in ./.conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning->-r requirements.txt (line 18)) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in ./.conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning->-r requirements.txt (line 18)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in ./.conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning->-r requirements.txt (line 18)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in ./.conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning->-r requirements.txt (line 18)) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning->-r requirements.txt (line 18)) (1.18.3)\n",
            "Requirement already satisfied: decorator in ./.conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements.txt (line 14)) (5.1.1)\n",
            "Requirement already satisfied: exceptiongroup in ./.conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements.txt (line 14)) (1.2.2)\n",
            "Requirement already satisfied: jedi>=0.16 in ./.conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements.txt (line 14)) (0.19.2)\n",
            "Requirement already satisfied: matplotlib-inline in ./.conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements.txt (line 14)) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in ./.conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements.txt (line 14)) (4.9.0)\n",
            "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./.conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements.txt (line 14)) (3.0.48)\n",
            "Requirement already satisfied: pygments>=2.4.0 in ./.conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements.txt (line 14)) (2.18.0)\n",
            "Requirement already satisfied: stack_data in ./.conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements.txt (line 14)) (0.6.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d->-r requirements.txt (line 14)) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in ./.conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d->-r requirements.txt (line 14)) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in ./.conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d->-r requirements.txt (line 14)) (0.22.3)\n",
            "Requirement already satisfied: platformdirs>=2.5 in ./.conda/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat>=5.7.0->open3d->-r requirements.txt (line 14)) (4.3.6)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in ./.conda/lib/python3.10/site-packages (from plotly>=5.0.0->dash>=2.6.0->open3d->-r requirements.txt (line 14)) (9.0.0)\n",
            "Requirement already satisfied: zipp>=3.20 in ./.conda/lib/python3.10/site-packages (from importlib-metadata->dash>=2.6.0->open3d->-r requirements.txt (line 14)) (3.21.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./.conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements.txt (line 14)) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in ./.conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements.txt (line 14)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in ./.conda/lib/python3.10/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements.txt (line 14)) (0.2.13)\n",
            "Requirement already satisfied: executing>=1.2.0 in ./.conda/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements.txt (line 14)) (2.1.0)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in ./.conda/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements.txt (line 14)) (3.0.0)\n",
            "Requirement already satisfied: pure_eval in ./.conda/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements.txt (line 14)) (0.2.3)\n",
            "Downloading torch_tb_profiler-0.4.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-tb-profiler\n",
            "Successfully installed torch-tb-profiler-0.4.3\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -U -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check if we have GPU support, and if not, warn the user.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA is available!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import warnings\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA is available!\")\n",
        "else:\n",
        "    warnings.warn(\"CUDA is not available. Running on CPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Settings\n",
        "\n",
        "All our settings are here for convenience.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
            "[Open3D INFO] WebRTC GUI backend enabled.\n",
            "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
            "Seed: 3594080470\n"
          ]
        }
      ],
      "source": [
        "from pole_gen.models import UtilityPoleLabel\n",
        "import secrets\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import warnings\n",
        "from utils.logging import warning_format\n",
        "\n",
        "# Configuration\n",
        "n_points: int = 1000\n",
        "classes: list = [l.name for l in UtilityPoleLabel]\n",
        "n_classes: int = len(classes)\n",
        "seed: int = secrets.randbits(32)\n",
        "torch.set_float32_matmul_precision(\"medium\")\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "print(f\"Seed: {seed}\")\n",
        "\n",
        "# Warning format\n",
        "warnings.formatwarning = warning_format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preparing our Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training & Evaluation Data\n",
        "\n",
        "This data is procedurally generated.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data directory found. Using existing training data.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2d8c0c5bb91b4871aa6d8bfb3d053430",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Checking dataset...:   0%|          | 0/2000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Splitting dataset into training and validation sets...\n",
            "Training dataset size: 1600\n",
            "Validation dataset size: 400\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pole_gen.data import generate_data\n",
        "from models.dataset import PointCloudDataset\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "\n",
        "out_path: str = os.path.join(\"data\", \"train\")\n",
        "eval_split: float = 0.2\n",
        "\n",
        "if not os.path.exists(out_path) or len(os.listdir(out_path)) == 0:\n",
        "    print(\"Directory is empty or does not exist. New testing data will be generated.\")\n",
        "    os.makedirs(out_path)\n",
        "    generate_data(\n",
        "        n_samples=2000,\n",
        "        n_points=n_points,\n",
        "        out_dir=out_path,\n",
        "        jitter=0.02,\n",
        "    )\n",
        "else:\n",
        "    print(\"Data directory found. Using existing training data.\")\n",
        "\n",
        "generated_dataset = PointCloudDataset(\n",
        "    file_paths=[os.path.join(out_path, f) for f in os.listdir(out_path)],\n",
        "    n_points=n_points,\n",
        ")\n",
        "\n",
        "for sample in tqdm(generated_dataset, desc=\"Checking dataset...\"):\n",
        "    n = len(sample[0].numpy())\n",
        "    if n != n_points:\n",
        "        warnings.warn(\n",
        "            f\"A sample has {n} points (Expected {n_points}). Check if the data was generated correctly!\"\n",
        "        )\n",
        "        break\n",
        "    label = sample[1].numpy()[0]\n",
        "    if label < 0 or label >= n_classes:\n",
        "        warnings.warn(\n",
        "            f\"Invalid label {label} found in the dataset. Check if the data was generated correctly!\"\n",
        "        )\n",
        "        break\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "print(\"Splitting dataset into training and validation sets...\")\n",
        "train_size = int((1 - eval_split) * len(generated_dataset))\n",
        "val_size = len(generated_dataset) - train_size\n",
        "train_dataset, eval_dataset = torch.utils.data.random_split(\n",
        "    generated_dataset, [train_size, val_size]\n",
        ")\n",
        "\n",
        "del generated_dataset\n",
        "\n",
        "\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(eval_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing Data\n",
        "\n",
        "This data is manually labeled, real-world laser scanned data. We will fetch this remotely if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing data directory found. Using existing testing data.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dbf390445b2140fb9e0bdb2051806d92",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Checking dataset...:   0%|          | 0/91 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing dataset size: 91\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from electrical_poles.data import download_data\n",
        "from models.dataset import PointCloudDataset\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "test_data_path = os.path.join(\"data\", \"test\")\n",
        "\n",
        "if not os.path.exists(test_data_path) or len(os.listdir(test_data_path)) == 0:\n",
        "    print(\n",
        "        \"Testing data directory is empty or does not exist. New testing data will be downloaded.\"\n",
        "    )\n",
        "    os.makedirs(test_data_path)\n",
        "    download_data(out_dir=test_data_path)\n",
        "else:\n",
        "    print(\"Testing data directory found. Using existing testing data.\")\n",
        "\n",
        "test_dataset = PointCloudDataset(\n",
        "    file_paths=[os.path.join(test_data_path, f) for f in os.listdir(test_data_path)],\n",
        "    n_points=n_points,\n",
        ")\n",
        "\n",
        "for sample in tqdm(test_dataset, desc=\"Checking dataset...\"):\n",
        "    n = len(sample[0].numpy())\n",
        "    if n != n_points:\n",
        "        warnings.warn(\n",
        "            f\"A sample has {n} points (Expected {n_points}). Check if the data was generated correctly!\"\n",
        "        )\n",
        "        break\n",
        "    label = sample[1].numpy()[0]\n",
        "    if label < 0 or label >= n_classes:\n",
        "        warnings.warn(\n",
        "            f\"Invalid label {label} found in the dataset. Check if the data was generated correctly!\"\n",
        "        )\n",
        "        break\n",
        "\n",
        "print(f\"Testing dataset size: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training Segmentation\n",
        "\n",
        "Now we can train our segmenter with our data, or load a pre-existing one if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Trainer will use only 1 of 8 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=8)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
            "\n",
            "  | Name     | Type     | Params | Mode \n",
            "----------------------------------------------\n",
            "0 | pointnet | PointNet | 1.7 M  | train\n",
            "----------------------------------------------\n",
            "1.7 M     Trainable params\n",
            "0         Non-trainable params\n",
            "1.7 M     Total params\n",
            "6.683     Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f7a8bdd746e74d259735903532d4c2df",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: |                                                                                            …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd46a891c81b4799baece32416085065",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |                                                                                                   …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "../aten/src/ATen/native/cuda/Loss.cu:240: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "File \u001b[0;32m~/research-project-python/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
            "File \u001b[0;32m~/research-project-python/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:579\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    575\u001b[0m     ckpt_path,\n\u001b[1;32m    576\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    577\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m )\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
            "File \u001b[0;32m~/research-project-python/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:986\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 986\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
            "File \u001b[0;32m~/research-project-python/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1030\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1030\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/research-project-python/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
            "File \u001b[0;32m~/research-project-python/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/research-project-python/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n",
            "File \u001b[0;32m~/research-project-python/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:250\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m     batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[0;32m~/research-project-python/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:190\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n",
            "File \u001b[0;32m~/research-project-python/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:268\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n",
            "File \u001b[0;32m~/research-project-python/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:159\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 159\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
            "File \u001b[0;32m~/research-project-python/.conda/lib/python3.10/site-packages/pytorch_lightning/core/module.py:1308\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;124;03mthe optimizer.\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1306\u001b[0m \n\u001b[1;32m   1307\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1308\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/research-project-python/.conda/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py:153\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n",
            "File \u001b[0;32m~/research-project-python/.conda/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:238\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/research-project-python/.conda/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py:122\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/research-project-python/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:69\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/research-project-python/.conda/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n",
            "File \u001b[0;32m~/research-project-python/.conda/lib/python3.10/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
            "File \u001b[0;32m~/research-project-python/.conda/lib/python3.10/site-packages/torch/optim/adam.py:121\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 121\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n",
            "File \u001b[0;32m~/research-project-python/.conda/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py:108\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03mhook is called.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m \n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n",
            "File \u001b[0;32m~/research-project-python/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:144\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
            "File \u001b[0;32m~/research-project-python/.conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/research-project-python/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:129\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 129\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/research-project-python/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:317\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[0;32m--> 317\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n",
            "File \u001b[0;32m~/research-project-python/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:311\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 311\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
            "File \u001b[0;32m~/research-project-python/.conda/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:390\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/research-project-python/models/pointnet.py:228\u001b[0m, in \u001b[0;36mPointNetLightningModel.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    227\u001b[0m pred_choice \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 228\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[43mpred_choice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    230\u001b[0m result \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m\"\u001b[39m: acc}\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpointnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PointNetLightningModel\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPointNetLightningModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_classes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/pointnet/checkpoints\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers_per_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/research-project-python/utils/ai.py:52\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataset, eval_dataset, test_dataset, epochs, checkpoint_path, workers_per_dataloader)\u001b[0m\n\u001b[1;32m     29\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m     30\u001b[0m     test_dataset,\n\u001b[1;32m     31\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m     32\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     33\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39mworkers_per_dataloader,\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     36\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     37\u001b[0m     min_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     38\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m     ],\n\u001b[1;32m     50\u001b[0m )\n\u001b[0;32m---> 52\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataloader\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/research-project-python/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:543\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/research-project-python/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:60\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     59\u001b[0m     _interrupt(trainer, exception)\n\u001b[0;32m---> 60\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_teardown\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# teardown might access the stage so we reset it after\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/research-project-python/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1009\u001b[0m, in \u001b[0;36mTrainer._teardown\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_teardown\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This is the Trainer's internal teardown, unrelated to the `teardown` hooks in LightningModule and Callback;\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;124;03m    those are handled by :meth:`_call_teardown_hook`.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1009\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mteardown\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1010\u001b[0m     loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_active_loop\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;66;03m# loop should never be `None` here but it can because we don't know the trainer stage with `ddp_spawn`\u001b[39;00m\n",
            "File \u001b[0;32m~/research-project-python/.conda/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:535\u001b[0m, in \u001b[0;36mStrategy.teardown\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    534\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: moving model to CPU\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 535\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mteardown()\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/research-project-python/.conda/lib/python3.10/site-packages/lightning_fabric/utilities/device_dtype_mixin.py:82\u001b[0m, in \u001b[0;36m_DeviceDtypeModuleMixin.cpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"See :meth:`torch.nn.Module.cpu`.\"\"\"\u001b[39;00m\n\u001b[1;32m     81\u001b[0m _update_properties(\u001b[38;5;28mself\u001b[39m, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/research-project-python/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:954\u001b[0m, in \u001b[0;36mModule.cpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcpu\u001b[39m(\u001b[38;5;28mself\u001b[39m: T) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    946\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the CPU.\u001b[39;00m\n\u001b[1;32m    947\u001b[0m \n\u001b[1;32m    948\u001b[0m \u001b[38;5;124;03m    .. note::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 954\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/research-project-python/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m~/research-project-python/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "    \u001b[0;31m[... skipping similar frames: Module._apply at line 797 (2 times)]\u001b[0m\n",
            "File \u001b[0;32m~/research-project-python/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m~/research-project-python/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
            "File \u001b[0;32m~/research-project-python/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:954\u001b[0m, in \u001b[0;36mModule.cpu.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcpu\u001b[39m(\u001b[38;5;28mself\u001b[39m: T) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    946\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the CPU.\u001b[39;00m\n\u001b[1;32m    947\u001b[0m \n\u001b[1;32m    948\u001b[0m \u001b[38;5;124;03m    .. note::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 954\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "from utils.ai import train\n",
        "from models.pointnet import PointNetLightningModel\n",
        "\n",
        "model = train(\n",
        "    model=PointNetLightningModel(task=\"seg\", num_classes=n_classes),\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    test_dataset=test_dataset,\n",
        "    checkpoint_path=\"data/pointnet/checkpoints\",\n",
        "    workers_per_dataloader=4,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
